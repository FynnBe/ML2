{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "name": "",
  "signature": "sha256:a2e547718822532d9a5e7789c56c13dad85e88bd0404e1711582173547eed92a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "from sklearn.datasets import load_digits\n",
      "from sklearn import cross_validation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2 Loading the Dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "digits = load_digits()\n",
      "\n",
      "data         = digits['data']\n",
      "images       = digits['images']\n",
      "target       = digits['target']\n",
      "target_names = digits['target_names']\n",
      "\n",
      "\n",
      "\n",
      "X = data[np.logical_or(target[:] == 3, target[:] == 8)]\n",
      "y = target[np.logical_or(target[:] == 3, target[:] == 8)]\n",
      "#y = y.reshape((y.size, 1))\n",
      "y[y==3] = 1\n",
      "y[y==8] = -1\n",
      "\n",
      "print X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(357, 64)\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2.1 Basic Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(x):\n",
      "    return 1. / (1. + np.exp(-x))\n",
      "\n",
      "def gradient_i(beta, X, y):\n",
      "    grad = (1 - sigmoid(y*X.dot(beta))) * (-y*X.T)\n",
      "    return grad\n",
      "    \n",
      "def gradient(beta, X, y):\n",
      "    return np.sum((np.ones(len(y)) - sigmoid(y * np.dot(X, beta)))*(-y * X.T), axis=1) / len(y)\n",
      "\n",
      "def predict(beta, X):\n",
      "    return np.sign(X.dot(beta))\n",
      "\n",
      "def zero_one_loss(y_pred, y_gt):\n",
      "    return sum(np.abs(y_pred - y_gt)) / 2.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "shapes (4,5) and (1,2) not aligned: 5 (dim 1) != 1 (dim 0)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-58-383c7a4e0109>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-58-383c7a4e0109>\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(beta, X, y)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: shapes (4,5) and (1,2) not aligned: 5 (dim 1) != 1 (dim 0)"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2.2 Optimization Method"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We chose to sample without resampling, because N is small compared to the number of iterations and therefore no sample should be picked twice."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gradient_descent(X, y, **kwargs):\n",
      "    T = kwargs.get('T', 10)\n",
      "    tau = kwargs.get('tau', 0.001)\n",
      "    beta0 = kwargs.get('beta0', np.zeros((64,)))\n",
      "                \n",
      "    beta_k = beta0\n",
      "    for t in range(T):\n",
      "        grad = gradient(beta_k, X, y)\n",
      "                         \n",
      "        beta_k = beta_k - tau*grad\n",
      "        \n",
      "        yield beta_k\n",
      "        \n",
      "    return\n",
      "\n",
      "def stochastic_gradient_descent(X, y, **kwargs):\n",
      "    T = kwargs.get('T', 150)\n",
      "    tau = kwargs.get('tau', 0.001)\n",
      "    gamma = kwargs.get('gamma', 0.0001)\n",
      "    beta_k = kwargs.get('beta0', np.zeros((64,)))\n",
      "    \n",
      "    sample_batch = np.random.permutation(len(y))[:T]\n",
      "    for t in range(T):\n",
      "        sample = sample_batch[t]\n",
      "        grad = gradient_i(beta_k, X[sample,:], y[sample])\n",
      "        \n",
      "        tau_k = tau / (1 + gamma*t)**0.75\n",
      "            \n",
      "        beta_k = beta_k - tau_k*grad\n",
      "        \n",
      "        yield beta_k\n",
      "        \n",
      "    return\n",
      "\n",
      "def sg_minibatch(X, y, **kwargs):\n",
      "    T = kwargs.get('T', 150)\n",
      "    tau = kwargs.get('tau', 0.001)\n",
      "    gamma = kwargs.get('gamma', 0.0001)\n",
      "    beta_k = kwargs.get('beta0', np.zeros((64,)))\n",
      "    batch_size = kwargs.get('batch_size', 10)\n",
      "    \n",
      "    for t in range(T):\n",
      "        batch = np.random.randint(X.shape[0], size=batch_size)\n",
      "        \n",
      "        grad = gradient(beta_k, X[batch,:], y[batch])\n",
      "        \n",
      "        tau_k = tau / (1 + gamma*t)\n",
      "        \n",
      "        beta_k = beta_k - tau_k*grad\n",
      "        \n",
      "        yield beta_k\n",
      "        \n",
      "    return\n",
      "\n",
      "def sg_momentum(X, y, **kwargs):\n",
      "    T = kwargs.get('T', 150)\n",
      "    tau = kwargs.get('tau', 0.001)\n",
      "    gamma = kwargs.get('gamma', 0.0001)\n",
      "    beta_k = kwargs.get('beta0', np.zeros((64,)))\n",
      "    mu = kwargs.get('mu', 0.1)\n",
      "    \n",
      "    g_t = 0.\n",
      "    \n",
      "    sample_batch = np.random.permutation(len(y))[:T]\n",
      "    for t in range(T):\n",
      "        sample = sample_batch[t]\n",
      "        \n",
      "        grad = gradient_i(beta_k, X[sample,:], y[sample])\n",
      "        \n",
      "        tau_k = tau / (1 + gamma*t)\n",
      "        \n",
      "        g_t = mu*g_t + (1-mu)*grad\n",
      "        beta_k = beta_k - tau_k*g_t\n",
      "        \n",
      "        yield beta_k\n",
      "        \n",
      "    return\n",
      "\n",
      "def average_stochastic_gradient(X, y, **kwargs):\n",
      "    T = kwargs.get('T', 150)\n",
      "    tau = kwargs.get('tau', 0.001)\n",
      "    gamma = kwargs.get('gamma', 0.0001)\n",
      "    beta_t = kwargs.get('beta0', np.zeros((64,)))\n",
      "    mu = kwargs.get('mu', 0.1)\n",
      "    \n",
      "    g_t = 0.\n",
      "    \n",
      "    sample_batch = np.random.permutation(len(y))[:T]\n",
      "    for t in range(T):\n",
      "        sample = sample_batch[t]\n",
      "        tau_t = tau / (1 + gamma*t)\n",
      "        \n",
      "        grad = gradient_i(g_t, X[sample,:], y[sample])\n",
      "        \n",
      "        g_t = g_t - tau_t*grad\n",
      "        beta_t = (1-mu)*beta_t + mu*g_t\n",
      "        \n",
      "        yield beta_t\n",
      "        \n",
      "    return\n",
      "\n",
      "\n",
      "def stochastic_average_gradient(X, y, **kwargs):\n",
      "    # TODO: Unklar wie der funktioniert weil i \\in M und i' \\in D ...\n",
      "    # passt nicht wenn immer nur ein element in d_t geandert wird.\n",
      "    \n",
      "    T = kwargs.get('T', 150)\n",
      "    tau = kwargs.get('tau', 0.001)\n",
      "    gamma = kwargs.get('gamma', 0.0001)\n",
      "    beta_t = kwargs.get('beta0', np.zeros((64,)))\n",
      "\n",
      "    N = X.shape[0]\n",
      "    D = X.shape[1]\n",
      "    \n",
      "    d_t = np.zeros((N))\n",
      "    \n",
      "    sample_batch = np.random.permutation(len(y))[:T]\n",
      "    for t in range(T):\n",
      "        sample = sample_batch[t]\n",
      "                \n",
      "        d_t[sample] = gradient_i(beta_k, X[sample,:], y[sample])\n",
      "        \n",
      "        tau_t = tau / (1 + gamma*t)\n",
      "        \n",
      "        beta_t = beta_t - tau_t*d_t\n",
      "    \n",
      "        yield beta_t\n",
      "\n",
      "    return\n",
      "\n",
      "\n",
      "def dual_coordinate_ascent(X, y, **kwargs):\n",
      "    T = kwargs.get('T', 150)\n",
      "    tau = kwargs.get('tau', 0.001)\n",
      "    gamma = kwargs.get('gamma', 0.0001)\n",
      "    beta_t = kwargs.get('beta0', np.zeros((64,)))\n",
      "    \n",
      "    alpha_t = np.random.rand()\n",
      "    beta_t = alpha_t * np.sum(np.multiply(y, X.T).T, axis=0)\n",
      "    \n",
      "    sample_batch = np.random.permutation(len(y))[:T]\n",
      "    for t in range(T):\n",
      "        sample = sample_batch[t]\n",
      "        \n",
      "        alpha_tp1 = np.clip(alpha_t - y[sample]*X[sample, :].dot(beta_t) / (X[sample].dot(X[sample].T)), \n",
      "                                      a_min=0., a_max=1.)\n",
      "                \n",
      "        beta_t = beta_t + (alpha_tp1 - alpha_t)*y[sample]*(X[sample,:])\n",
      "        alpha_t = alpha_tp1\n",
      "        \n",
      "        yield beta_t\n",
      "        \n",
      "    return\n",
      "\n",
      "def weighted_least_squares(X, y, **kwargs):\n",
      "    T = kwargs.get('T', 150)\n",
      "    tau = kwargs.get('tau', 0.001)\n",
      "    gamma = kwargs.get('gamma', 0.0001)\n",
      "    beta_t = kwargs.get('beta0', np.zeros((64,)))\n",
      "    \n",
      "    \n",
      "    M = X.shape[0]\n",
      "    \n",
      "    for t in range(T):\n",
      "        z_t = X.dot(beta_t)\n",
      "        V_t = np.diag(np.sqrt(1./M * sigmoid(z_t)*(np.ones(M) - sigmoid(z_t))))\n",
      "        y_tilde_t = y / sigmoid(y*z_t)\n",
      "        z_tilde_t = (z_t + y_tilde_t).dot(V_t)\n",
      "        X_tilde_t = X.T.dot(V_t).T\n",
      "        \n",
      "        X_tilde_t = np.nan_to_num(X_tilde_t)\n",
      "        \n",
      "        beta_t, _, _, _ = np.linalg.lstsq(X_tilde_t, z_tilde_t)\n",
      "        \n",
      "        yield beta_t\n",
      "        \n",
      "    return\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2.3 Comparison"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# NOTE: Allgemein riesige Varianz der Ergebnisse, von daher schwierig\n",
      "# zu sagen welche parameter am besten sind"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cv_learning_rate(method, **kwargs):\n",
      "\n",
      "    zero_one_loss_sum = 0.\n",
      "    beta = np.zeros((64,))\n",
      "\n",
      "    cv = cross_validation.KFold(y.shape[0], n_folds=10)\n",
      "    for train_index, test_index in cv:\n",
      "        X_train, X_test = X[train_index], X[test_index]\n",
      "        y_train, y_test = y[train_index], y[test_index]\n",
      "        \n",
      "        for beta in method(X_train, y_train, **kwargs):\n",
      "            pass\n",
      "        \n",
      "        y_pred = predict(beta, X_test)\n",
      "        zero_one_loss_sum += zero_one_loss(y_pred, y_test)\n",
      "        \n",
      "    return zero_one_loss_sum"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Gradient Descent"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for tau in [0.001, 0.01, 0.1]:\n",
      "    options = {'T': 10, 'tau': tau}\n",
      "    loss_sum = cv_learning_rate(gradient_descent, **options)\n",
      "    print 'tau={}:\\t{}'.format(tau, loss_sum)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tau=0.001:\t17.0\n",
        "tau=0.01:\t18.0\n",
        "tau=0.1:\t47.0\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Stochastic Gradient Descent"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(1)\n",
      "\n",
      "for tau in [0.001, 0.01, 0.1]:\n",
      "    for gamma in [0.0001, 0.001, 0.01]:\n",
      "        options = {'T': 150, 'tau': tau, 'gamma': gamma}\n",
      "        loss_sum = cv_learning_rate(stochastic_gradient_descent, **options)\n",
      "        print 'tau={:5}\\tgamma={:6}\\tLoss_sum={}'.format(tau, gamma, loss_sum)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tau=0.001\tgamma=0.0001\tLoss_sum=15.0\n",
        "tau=0.001\tgamma= 0.001\tLoss_sum=25.0\n",
        "tau=0.001\tgamma=  0.01\tLoss_sum=19.0\n",
        "tau= 0.01\tgamma=0.0001\tLoss_sum=23.0\n",
        "tau= 0.01\tgamma= 0.001\tLoss_sum=29.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tau= 0.01\tgamma=  0.01\tLoss_sum=15.0\n",
        "tau=  0.1\tgamma=0.0001\tLoss_sum=37.0\n",
        "tau=  0.1\tgamma= 0.001\tLoss_sum=15.0\n",
        "tau=  0.1\tgamma=  0.01\tLoss_sum=16.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "SG minibatch"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for tau in [0.001, 0.01, 0.1]:\n",
      "    for gamma in [0.0001, 0.001, 0.01]:\n",
      "        options = {'T': 150, 'tau': tau, 'gamma': gamma}\n",
      "        loss_sum = cv_learning_rate(sg_minibatch, **options)\n",
      "        print 'tau={:5}\\tgamma={:6}\\tLoss_sum={}'.format(tau, gamma, loss_sum)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tau=0.001\tgamma=0.0001\tLoss_sum=13.0\n",
        "tau=0.001\tgamma= 0.001\tLoss_sum=12.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tau=0.001\tgamma=  0.01\tLoss_sum=12.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tau= 0.01\tgamma=0.0001\tLoss_sum=14.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tau= 0.01\tgamma= 0.001\tLoss_sum=15.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tau= 0.01\tgamma=  0.01\tLoss_sum=12.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tau=  0.1\tgamma=0.0001\tLoss_sum=9.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tau=  0.1\tgamma= 0.001\tLoss_sum=12.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tau=  0.1\tgamma=  0.01\tLoss_sum=15.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "SG momentum"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for tau in [0.001, 0.01, 0.1]:\n",
      "    for gamma in [0.0001, 0.001, 0.01]:\n",
      "        for mu in [0.1, 0.2, 0.3]:\n",
      "            options = {'T': 150, 'tau': tau, 'gamma': gamma, 'mu': mu}\n",
      "            loss_sum = cv_learning_rate(sg_momentum, **options)\n",
      "            print 'tau={:5}\\tgamma={:6}\\tmu={:3}\\tLoss_sum={}'.format(tau, gamma, mu, loss_sum)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Average Stochastic Gradient"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for tau in [0.001, 0.01, 0.1]:\n",
      "    for gamma in [0.0001, 0.001, 0.01]:\n",
      "        for mu in [0.1, 0.2, 0.3]:\n",
      "            options = {'T': 150, 'tau': tau, 'gamma': gamma, 'mu': mu}\n",
      "            loss_sum = cv_learning_rate(average_stochastic_gradient, **options)\n",
      "            print 'tau={:5}\\tgamma={:6}\\tmu={:3}\\tLoss_sum={}'.format(tau, gamma, mu, loss_sum)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Stochastic Average Gradient"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# FIXME!!!\n",
      "\"\"\"\n",
      "for tau in [0.001, 0.01, 0.1]:\n",
      "    for gamma in [0.0001, 0.001, 0.01]:\n",
      "        options = {'T': 150, 'tau': tau, 'gamma': gamma}\n",
      "        loss_sum = cv_learning_rate(stochastic_average_gradient, **options)\n",
      "        print 'tau={:5}\\tgamma={:6}\\tLoss_sum={}'.format(tau, gamma, loss_sum)\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Dual Coordinate Ascent (no free parameters)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "options = {'T': 100, 'tau': tau, 'gamma': gamma, 'mu': mu}\n",
      "loss_sum = cv_learning_rate(dual_coordinate_ascent, **options)\n",
      "print 'Loss_sum={}'.format(loss_sum)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Weighted Least Squares (again no free parameters)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# NOTE: ab gewisser Iterationsanzahl (z.b. T=10) kommen bei X^tilde nans rein...\n",
      "options = {'T': 1, 'tau': tau, 'gamma': gamma, 'mu': mu}\n",
      "loss_sum = cv_learning_rate(weighted_least_squares, **options)\n",
      "print 'Loss_sum={}'.format(loss_sum)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Speed"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.3, random_state=0)\n",
      "\n",
      "N = X_train.shape[0]\n",
      "D = X_train.shape[1]\n",
      "T_det = 10\n",
      "T_stoch = 150\n",
      "B = 10\n",
      "\n",
      "fig = plt.figure(figsize=(14,6))\n",
      "ax1 = fig.add_subplot(211)\n",
      "ax1.set_title('Training Error')\n",
      "ax2 = fig.add_subplot(212)\n",
      "ax2.set_title('Test Error')\n",
      "\n",
      "ax1.set_xlim(0, 15e3)\n",
      "ax2.set_xlim(0, 15e3)\n",
      "\n",
      "# Gradient Descent\n",
      "tau=0.001\n",
      "options = {'T': T_det, 'tau': tau}\n",
      "train_error = [zero_one_loss(predict(beta, X_train), y_train) for beta in gradient_descent(X_train, y_train)]\n",
      "test_error = [zero_one_loss(predict(beta, X_test), y_test) for beta in gradient_descent(X_train, y_train)]\n",
      "\n",
      "ts = np.linspace(1, T_det, T_det)*N*D\n",
      "ax1.plot(ts, train_error, label='GD')\n",
      "ax2.plot(ts, test_error, label='GD')\n",
      "\n",
      "\n",
      "# Stochastic Gradient Descent\n",
      "train_error = [zero_one_loss(predict(beta, X_train), y_train) for beta in stochastic_gradient_descent(X_train, y_train)]\n",
      "test_error = [zero_one_loss(predict(beta, X_test), y_test) for beta in stochastic_gradient_descent(X_train, y_train)]\n",
      "\n",
      "ts = np.linspace(1, T_stoch, T_stoch)*D\n",
      "ax1.plot(ts, train_error, label='SGD')\n",
      "ax2.plot(ts, test_error, label='SGD')\n",
      "\n",
      "\n",
      "# SG minibatch\n",
      "train_error = [zero_one_loss(predict(beta, X_train), y_train) for beta in sg_minibatch(X_train, y_train)]\n",
      "test_error = [zero_one_loss(predict(beta, X_test), y_test) for beta in sg_minibatch(X_train, y_train)]\n",
      "\n",
      "ts = np.linspace(1, T_stoch, T_stoch)*B*D\n",
      "ax1.plot(ts, train_error, label='SG minibatch')\n",
      "ax2.plot(ts, test_error, label='SG minibatch')\n",
      "\n",
      "# SG momentum\n",
      "train_error = [zero_one_loss(predict(beta, X_train), y_train) for beta in sg_momentum(X_train, y_train)]\n",
      "test_error = [zero_one_loss(predict(beta, X_test), y_test) for beta in sg_momentum(X_train, y_train)]\n",
      "\n",
      "ts = np.linspace(1, T_stoch, T_stoch)*D\n",
      "ax1.plot(ts, train_error, label='SG momentum')\n",
      "ax2.plot(ts, test_error, label='SG momentum')\n",
      "\n",
      "# Average stochastic gradient\n",
      "train_error = [zero_one_loss(predict(beta, X_train), y_train) for beta in average_stochastic_gradient(X_train, y_train)]\n",
      "test_error = [zero_one_loss(predict(beta, X_test), y_test) for beta in average_stochastic_gradient(X_train, y_train)]\n",
      "\n",
      "ts = np.linspace(1, T_stoch, T_stoch)*D\n",
      "ax1.plot(ts, train_error, label='ASG')\n",
      "ax2.plot(ts, test_error, label='ASG')\n",
      "\n",
      "# Stochastic average gradient\n",
      "# TODO!\n",
      "\n",
      "\n",
      "# Dual Coordinate Ascent\n",
      "train_error = [zero_one_loss(predict(beta, X_train), y_train) for beta in dual_coordinate_ascent(X_train, y_train)]\n",
      "test_error = [zero_one_loss(predict(beta, X_test), y_test) for beta in dual_coordinate_ascent(X_train, y_train)]\n",
      "\n",
      "ts = np.linspace(1, T_stoch, T_stoch)*D\n",
      "ax1.plot(ts, train_error, label='DCA')\n",
      "ax2.plot(ts, test_error, label='DCA')\n",
      "\n",
      "# Reweighted Least Squares\n",
      "train_error = [zero_one_loss(predict(beta, X_train), y_train) for beta in weighted_least_squares(X_train, y_train)]\n",
      "test_error = [zero_one_loss(predict(beta, X_test), y_test) for beta in weighted_least_squares(X_train, y_train)]\n",
      "\n",
      "ts = np.linspace(1, T_stoch, T_stoch)*N**2*D\n",
      "ax1.plot(ts, train_error, label='IRLS')\n",
      "ax2.plot(ts, test_error, label='IRLS')\n",
      "\n",
      "\n",
      "\n",
      "ax1.legend()\n",
      "ax2.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}